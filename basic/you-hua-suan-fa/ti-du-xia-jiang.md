# 梯度下降

梯度下降法（Gradient Descent）是一种常用的优化算法，广泛应用于机器学习和深度学习中，用于找到函数的局部最小值。其基本思想是不断沿着函数的梯度（即导数）反方向更新参数，使得函数值逐步减小，从而接近最优解。

梯度下降法的基本步骤如下：

1. **选择初始点** $$x_0$$ 。
2. **设定学习率** $$\alpha$$，它控制每次迭代的步长。
3. **迭代更新** ( x )： $$x_{n+1} = x_n - \alpha \cdot f'(x_n)$$  其中 $$f'(x_n)$$ 是函数 $$f(x)$$ 在点 $$x_n$$ 的一阶导数。

这个过程会持续进行，直到收敛到一个足够接近的最优解点。

**例子：利用梯度下降法找到一元二次方程的最优解**

考虑一个简单的一元二次方程： $$f(x) = x^2 - 4x + 4$$&#x20;

1. **找出导数**： 一阶导数 $$f'(x) = 2x - 4$$
2. **选择一个初始点**： 设定初始点 $$x_0 = 0$$
3. **设定学习率** $$\alpha$$： 选择一个适当的学习率，比如 $$\alpha = 0.1$$
4. **应用梯度下降更新公式**： 利用梯度下降的更新公式： $$x_{n+1} = x_n - \alpha \cdot f'(x_n)$$
5.  **计算迭代步骤**：

    * 设 n = 0 ，初始点 $$( x_0 = 0 ) [ f'(x_0) = 2(0) - 4 = -4 ] [ x_1 = x_0 - \alpha \cdot f'(x_0) = 0 - 0.1 \cdot (-4) = 0.4 ]$$
    * 设 n = 1 ，新的点 $$( x_1 = 0.4 ) [ f'(x_1) = 2(0.4) - 4 = -3.2 ] [ x_2 = x_1 - \alpha \cdot f'(x_1) = 0.4 - 0.1 \cdot (-3.2) = 0.72 ]$$
    * 设 n = 2 ，新的点 $$( x_2 = 0.72 ) [ f'(x_2) = 2(0.72) - 4 = -2.56 ] [ x_3 = x_2 - \alpha \cdot f'(x_2) = 0.72 - 0.1 \cdot (-2.56) = 0.976 ]$$

    持续迭代，参数 ( x ) 会逐渐逼近最优解。

注意：学习率 $$\alpha$$  的选择非常重要。如果学习率太大，可能会跳过最优解，甚至不收敛；如果学习率太小，收敛速度会非常慢。因此，通常需要进行调试或使用自适应学习率等技术来优化。

总结：梯度下降法通过反复更新参数，沿着函数梯度的负方向逐步减小函数值，从而逼近最优解。在本例中的一元二次方程，逐步计算并更新参数，即可接近极小值点 ( x = 2 )。
